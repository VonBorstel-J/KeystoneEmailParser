
            return {}
        try:
            subject, from_address, body, attachments = parse_email(email_content)
            return {
                "email_metadata": {
                    "subject": subject,
                    "from": from_address,
                    "body": body,
                    "attachments": [att[0] for att in attachments],
                }
            }
        except ParsingError as pe:
            self.logger.error("ParsingError during Email Parsing stage: %s", pe)
            raise
        except Exception as e:
            self.logger.error("Error during Email Parsing stage: %s", e, exc_info=True)
            raise ParsingError(f"Email Parsing failed: {e}") from e
    def _stage_donut_parsing(
        self, document_image: Optional[Union[str, Image.Image]] = None
    ) -> Dict[str, Any]:
        if not document_image:
            self.logger.warning("No document image provided for Donut Parsing.")
            return {}
        self.logger.debug("Executing Donut Parsing stage.")
        try:
            if self.donut_model is None or self.donut_processor is None:
                self.logger.warning(
                    "Donut model or processor is not available. Skipping Donut Parsing."
                )
                return {}
            donut_output = perform_donut_parsing(
                document_image=document_image,
                processor=self.donut_processor,
                model=self.donut_model,
                device=self.device,
                logger=self.logger,
                config=self.config,
            )
            if not donut_output:
                self.logger.warning("Donut Parsing returned empty output.")
                return {}
            mapped_data = self.map_donut_output_to_schema(donut_output)
            return mapped_data
        except (ValueError, OSError) as e:
            self.logger.error("Error during Donut Parsing stage: %s", e)
            raise ParsingError(f"Donut Parsing failed: {e}") from e
        except Exception as e:
            self.logger.error(
                "Unexpected error during Donut Parsing stage: %s", e, exc_info=True
            )
            raise ParsingError(f"Donut Parsing failed: {e}") from e
    def _stage_text_extraction(
        self, email_content: Optional[str] = None
    ) -> Dict[str, Any]:
        if not email_content:
            self.logger.warning("No email content provided for Text Extraction.")
            return {}
        try:
            if self.llama_model is None:
                self.logger.warning(
                    "LLaMA model is not available. Skipping Text Extraction."
                )
                return {}
            prompt = self.llama_model["prompt_templates"].get("text_extraction", "")
            if not prompt:
                self.logger.warning("No prompt template for Text Extraction.")
                return {}
            parsing_result = perform_model_based_parsing(
                prompt, self.llama_model, self.logger
            )
            # Assuming 'structured_data' contains the extracted fields
            structured_data = parsing_result.get("structured_data", {})
            metadata = parsing_result.get("metadata", {})
            # Optionally, store metadata if needed
            self.logger.debug(f"Metadata from parsing: {metadata}")
            return structured_data
        except ParsingError as pe:
            self.logger.error("ParsingError during Text Extraction stage: %s", pe)
            raise
        except Exception as e:
            self.logger.error(
                "Unexpected error during Text Extraction stage: %s",
                e,
                exc_info=True,
            )
            raise ParsingError(f"Text Extraction failed: {e}") from e
    def _stage_validation(
        self,
        email_content: Optional[str] = None,
        parsed_data: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        if not email_content or not parsed_data:
            self.logger.warning(
                "Insufficient data for Validation. Skipping this stage."
            )
            return parsed_data
        self.logger.debug("Executing Validation stage.")
        try:
            prompt = self.llama_model["prompt_templates"].get("validation", "")
            if not prompt:
                self.logger.warning("No prompt template for Validation.")
                return parsed_data
            # Assuming validation might update 'parsed_data' with validation results
            validation_result = perform_model_based_parsing(
                prompt, self.llama_model, self.logger
            )
            # Merge validation results into parsed_data
            parsed_data.update(validation_result.get("structured_data", {}))
            parsed_data.update(validation_result.get("metadata", {}))
            return parsed_data
        except (ValueError, OSError) as e:
            self.logger.error("ValidationError during Validation stage: %s", e)
            parsed_data["validation_issues"] = parsed_data.get(
                "validation_issues", []
            ) + [str(e)]
            raise ValidationError(f"Validation failed: {e}") from e
        except Exception as e:
            self.logger.error(
                "Unexpected error during Validation stage: %s",
                e,
                exc_info=True,
            )
            parsed_data["validation_issues"] = parsed_data.get(
                "validation_issues", []
            ) + [str(e)]
            raise ValidationError(f"Validation failed: {e}") from e
    def _stage_summarization(
        self,
        email_content: Optional[str] = None,
        parsed_data: Optional[Dict[str, Any]] = None,
    ) -> None:
        if not email_content or not parsed_data:
            self.logger.warning(
                "Insufficient data for Summarization. Skipping this stage."
            )
            return
        try:
            if self.llama_model is None:
                self.logger.warning(
                    "LLaMA model is not available. Skipping Summarization."
                )
                return
            prompt = self.llama_model["prompt_templates"].get("summarization", "")
            if not prompt:
                self.logger.warning("No prompt template for Summarization.")
                return
            summary = perform_model_based_parsing(
                prompt, self.llama_model["model"], self.logger
            )
            if summary:
                parsed_data["summary"] = summary
                self.logger.debug("Summarization Result: %s", parsed_data["summary"])
        except ParsingError as pe:
            self.logger.error("ParsingError during Summarization stage: %s", pe)
            raise
        except Exception as e:
            self.logger.error(
                "Unexpected error during Summarization stage: %s", e, exc_info=True
            )
            raise ParsingError(f"Summarization failed: {e}") from e
    def _stage_post_processing(
        self, parsed_data: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        if not parsed_data:
            self.logger.warning(
                "No parsed data available for Post Processing. Skipping this stage."
            )
            return {}
        try:
            return post_process_parsed_data(parsed_data, self.logger)
        except (ValueError, OSError) as e:
            self.logger.error("Error during Post Processing stage: %s", e)
            parsed_data["validation_issues"] = parsed_data.get(
                "validation_issues", []
            ) + [str(e)]
            raise ParsingError(f"Post Processing failed: {e}") from e
        except Exception as e:
            self.logger.error(
                "Unexpected error during Post Processing stage: %s", e, exc_info=True
            )
            parsed_data["validation_issues"] = parsed_data.get(
                "validation_issues", []
            ) + [str(e)]
            raise ParsingError(f"Post Processing failed: {e}") from e
    def _stage_json_validation(
        self, parsed_data: Optional[Dict[str, Any]] = None
    ) -> None:
        if not parsed_data:
            self.logger.warning(
                "No parsed data available for JSON Validation. Skipping this stage."
            )
            return
        try:
            is_valid, error_message = validate_json(parsed_data)
            if is_valid:
                self.logger.info("JSON validation passed.")
            else:
                self.logger.error("JSON validation failed: %s", error_message)
                parsed_data["validation_issues"] = parsed_data.get(
                    "validation_issues", []
                ) + [error_message]
        except ValidationError as ve:
            self.logger.error("ValidationError during JSON validation: %s", ve)
            parsed_data["validation_issues"] = parsed_data.get(
                "validation_issues", []
            ) + [str(ve)]
            raise
        except Exception as e:
            self.logger.error(
                "Unexpected error during JSON validation: %s", e, exc_info=True
            )
            parsed_data["validation_issues"] = parsed_data.get(
                "validation_issues", []
            ) + [str(e)]
            raise ValidationError(f"JSON Validation failed: {e}") from e
    def map_donut_output_to_schema(self, donut_json: Dict[str, Any]) -> Dict[str, Any]:
        mapped_data: Dict[str, Any] = {}
        try:
            field_mapping = {
                "policy_number": (ADJUSTER_INFORMATION, "Policy #"),
                "claim_number": (REQUESTING_PARTY, "Carrier Claim Number"),
                "insured_name": (INSURED_INFORMATION, "Name"),
                "loss_address": (INSURED_INFORMATION, "Loss Address"),
                "adjuster_name": (ADJUSTER_INFORMATION, "Adjuster Name"),
                "adjuster_phone": (ADJUSTER_INFORMATION, "Adjuster Phone Number"),
                "adjuster_email": (ADJUSTER_INFORMATION, "Adjuster Email"),
                "date_of_loss": (
                    ASSIGNMENT_INFORMATION,
                    "Date of Loss/Occurrence",
                ),
                "cause_of_loss": (ASSIGNMENT_INFORMATION, "Cause of loss"),
                "loss_description": (ASSIGNMENT_INFORMATION, "Loss Description"),
                "inspection_type": (ASSIGNMENT_INFORMATION, "Inspection type"),
                "repair_progress": (
                    ASSIGNMENT_INFORMATION,
                    "Repair or Mitigation Progress",
                ),
                "residence_occupied": (
                    ASSIGNMENT_INFORMATION,
                    "Residence Occupied During Loss",
                ),
                "someone_home": (
                    ASSIGNMENT_INFORMATION,
                    "Was Someone home at time of damage",
                ),
                "type": (ASSIGNMENT_INFORMATION, "Type"),
                "additional_instructions": (
                    "Additional details/Special Instructions",
                    "Details",
                ),
                "attachments": ("Attachment(s)", "Files"),
                "owner_tenant": (
                    INSURED_INFORMATION,
                    "Is the insured an Owner or a Tenant of the loss location?",
                ),
            }
            for item in donut_json.get("form", []):
                field_name = item.get("name")
                field_value = item.get("value")
                if field_name in field_mapping:
                    section, qb_field = field_mapping[field_name]
                    if field_name in ["residence_occupied", "someone_home"]:
                        field_value = field_value.lower() in ["yes", "true", "1"]
                    mapped_data.setdefault(section, {}).setdefault(qb_field, []).append(
                        field_value
                    )
            return mapped_data
        except ValueError as ve:
            self.logger.error(f"ValueError during Donut mapping: {ve}")
            raise ValidationError(f"Donut mapping failed: {ve}") from ve
        except Exception as e:
            self.logger.error(f"Error during Donut mapping: {e}", exc_info=True)
            raise ValidationError(f"Donut mapping failed: {e}") from e
    def cleanup_resources(self):
        """Clean up resources including the event loop."""
        self.logger.info("Starting resource cleanup.")
        cleanup_errors = []
        try:
            with self.lock:
                self._cleanup_models(cleanup_errors)
                self._cleanup_executor(cleanup_errors)
            if cleanup_errors:
                self.logger.warning("Cleanup completed with errors: %s", cleanup_errors)
            else:
                self.logger.info("Cleanup completed successfully")
        except Exception as e:
            self.logger.error("Fatal error during cleanup: %s", e, exc_info=True)
            raise
    def _cleanup_models(self, cleanup_errors: List[str]):
        models_to_cleanup = [
            (self.donut_model, "Donut model"),
            (self.donut_processor, "Donut processor"),
            (self.llama_model, "LLaMA model"),
        ]
        for model, name in models_to_cleanup:
            if model is not None:
                try:
                    if hasattr(model, "model"):
                        model.model.cpu()
                    elif hasattr(model, "to"):
                        model.to("cpu")
                except ValueError as ve:
                    error_msg = f"ValueError moving {name} to CPU: {ve}"
                    self.logger.error(error_msg)
                    cleanup_errors.append(error_msg)
                except OSError as oe:
                    error_msg = f"OSError moving {name} to CPU: {oe}"
                    self.logger.error(error_msg)
                    cleanup_errors.append(error_msg)
                except Exception as e:
                    error_msg = f"Unexpected error moving {name} to CPU: {e}"
                    self.logger.error(
                        "Unexpected error moving %s to CPU: %s",
                        name,
                        e,
                        exc_info=True,
                    )
                    cleanup_errors.append(error_msg)
    def _cleanup_executor(self, cleanup_errors: List[str]):
        if self.executor:
            try:
                self.executor.shutdown(wait=True)
                self.logger.debug("Executor shutdown successfully.")
            except Exception as e:
                error_msg = f"Error during executor shutdown: {e}"
                self.logger.error(error_msg)
                cleanup_errors.append(error_msg)
    def _unload_models(self):
        models_to_unload = [
            "donut_model",
            "donut_processor",
            "llama_model",
        ]
        for model_attr in models_to_unload:
            model = getattr(self, model_attr, None)
            if model is not None:
                delattr(self, model_attr)
                self.logger.debug(f"Unloaded {model_attr} from memory.")
        torch.cuda.empty_cache()
        self.logger.debug("Cleared CUDA cache.")
    def recover_from_failure(self, stage: str) -> bool:
        self.logger.warning("Attempting to recover from %s failure", stage)
        recoverable_stages = {
            "donut_parsing": lambda: self._recover_donut_parsing(),
            "text_extraction": lambda: self._recover_llama(),
            "validation": lambda: self._recover_llama(),
            "summarization": lambda: self._recover_llama(),
            "post_processing": None,
            "json_validation": None,
        }
        stage_key = stage.lower().replace(" ", "_")
        recovery_action = recoverable_stages.get(stage_key)
        if recovery_action:
            try:
                recovery_action()
                self._initialize_executor()
                self._initialize_models(self.input_type)
                health = self.health_check()
                recovery_successful = health.get(stage_key, False)
                if recovery_successful:
                    self.logger.info("Successfully recovered from %s failure", stage)
                else:
                    self.logger.warning(
                        "Recovery from %s failure was unsuccessful", stage
                    )
                return recovery_successful
            except (ValueError, OSError, TypeError) as e:
                self.logger.error("Error during recovery from %s: %s", stage, e)
                return False
            except Exception as e:
                self.logger.error(
                    "Unexpected error during recovery from %s: %s",
                    stage,
                    e,
                    exc_info=True,
                )
                return False
        self.logger.debug("No recovery method available for stage: %s", stage)
        return False
    def _recover_donut_parsing(self):
        self.donut_processor, self.donut_model = initialize_donut(
            self.logger, self.config
        )
    def _recover_llama(self):
        self.llama_model = initialize_model_parser(
            self.logger,
            self.config,
            prompt_templates=self._render_prompts(),
        )
    def validate_input(
        self,
        email_content: Optional[str] = None,
        document_image: Optional[Union[str, Image.Image]] = None,
    ) -> bool:
        if not email_content and not document_image:
            self.logger.error("No input provided.")
            return False
        if document_image and not isinstance(document_image, (str, Image.Image)):
            self.logger.error("Invalid document_image type: %s", type(document_image))
            return False
        if email_content and not isinstance(email_content, str):
            self.logger.error("Invalid email_content type: %s", type(email_content))
            return False
        return True
    def __enter__(self):
        return self
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup_resources()
    def get_performance_metrics(self) -> Dict[str, Any]:
        metrics = {
            "memory_usage": self._check_memory_usage(),
            "cpu_usage_percent": psutil.cpu_percent(interval=1),
            "model_status": self.health_check(),
            "active_threads": self.max_workers,
            "processing_times": {
                "donut_parsing": self.timeouts.get("donut_parsing", 60),
                "llama_text_extraction": self.timeouts.get("llama_text_extraction", 60),
                "llama_validation": self.timeouts.get("llama_validation", 45),
                "llama_summarization": self.timeouts.get("llama_summarization", 30),
                "post_processing": self.timeouts.get("post_processing", 30),
                "json_validation": self.timeouts.get("json_validation", 30),
            },
        }
        return metrics
    def _check_memory_usage(self) -> Dict[str, float]:
        memory_info = {}
        if torch.cuda.is_available():
            memory_info["cuda"] = {
                "allocated": torch.cuda.memory_allocated() / 1024**2,
                "cached": torch.cuda.memory_reserved() / 1024**2,
                "max_allocated": torch.cuda.max_memory_allocated() / 1024**2,
            }
        return memory_info
    def health_check(self) -> Dict[str, bool]:
        health = {
            "donut_parsing": self.donut_model is not None
            and self.donut_processor is not None,
            "llama_text_extraction": self.llama_model is not None,
            "llama_validation": self.llama_model is not None,
            "llama_summarization": self.llama_model is not None,
        }
        self.logger.debug("Health check status: %s", health)
        return health
    @property
    def max_workers(self) -> int:
        if not self.executor:
            return 0
        try:
            return getattr(self.executor, "_max_workers", 0)
        except AttributeError:
            self.logger.warning("Unable to get max workers count")
            return 0




# -------------------- C:\Users\jorda\OneDrive\Desktop\Quickbase Dev Work\KeystoneEmailParser\templates\index.html --------------------

<!-- templates/index.html -->
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="icon" href="{{ url_for('static', filename='favicon.ico') }}" />
    <title>Email Parsing App</title>
  </head>
  <body>
    <div id="root"></div>
  </body>
</html>




# -------------------- C:\Users\jorda\OneDrive\Desktop\Quickbase Dev Work\KeystoneEmailParser\webpack.config.js --------------------

// webpack.config.js
const path = require('path');
const MiniCssExtractPlugin = require('mini-css-extract-plugin');
const HtmlWebpackPlugin = require('html-webpack-plugin'); // To generate HTML files
module.exports = (env, argv) => {
  const isProduction = argv.mode === 'production';
  return {
    mode: isProduction ? 'production' : 'development',
    entry: {
      main: './frontend/index.js', // Updated entry point
    },
    output: {
      path: path.resolve(__dirname, 'static/dist'),
      filename: isProduction ? '[name].[contenthash].js' : '[name].js',
      chunkFilename: isProduction ? '[name].[contenthash].js' : '[name].js',
      publicPath: '/',
      clean: true,
    },
    devtool: isProduction ? 'source-map' : 'inline-source-map',
    devServer: {
      static: [
        {
          directory: path.join(__dirname, 'static'),
          publicPath: '/static',
        },
        {
          directory: path.join(__dirname, 'templates'),
          publicPath: '/',
        },
      ],
      compress: true,
      port: 8080,
      hot: true,
      historyApiFallback: {
        rewrites: [
          { from: /^\/static\/dist\/[a-z0-9]+\.[a-z0-9]+\.[a-z0-9]+\.(js|css)$/, to: (context) => context.parsedUrl.pathname },
        ],
      },
      proxy: {
        '/api': 'http://localhost:5000',
        '/socket.io': {
          target: 'http://localhost:5000',
          ws: true,
          changeOrigin: true,
          timeout: 60000, 
      proxyTimeout: 60000,
        },
      },
    },
    module: {
      rules: [
        {
          test: /\.(js|jsx)$/, // Handle JS and JSX files
          exclude: /node_modules/,
          use: {
            loader: 'babel-loader',
          },
        },
        {
          test: /\.css$/, // Handle CSS files
          use: [
            isProduction ? MiniCssExtractPlugin.loader : 'style-loader', // Extract CSS in production
            {
              loader: 'css-loader',
              options: {
                importLoaders: 1, // Number of loaders applied before CSS loader
              },
            },
            'postcss-loader', // Apply PostCSS transformations
          ],
        },
        {
          test: /\.(png|jpe?g|gif|svg)$/i, // Handle image assets
          type: 'asset/resource',
          generator: {
            filename: 'images/[hash][ext][query]', // Output images to images/ directory
          },
        },
      ],
    },
    resolve: {
      extensions: ['.js', '.jsx', '.css'], // Resolve these extensions
      alias: {
        '@': path.resolve(__dirname, 'frontend'),
        '@components': path.resolve(__dirname, 'frontend/components'),
        '@actions': path.resolve(__dirname, 'frontend/actions'),
        '@reducers': path.resolve(__dirname, 'frontend/reducers'),
        '@core': path.resolve(__dirname, 'frontend/core'),
        '@utils': path.resolve(__dirname, 'frontend/utils'),
        '@css': path.resolve(__dirname, 'frontend/static/css'),
      },
    },
    plugins: [
      new MiniCssExtractPlugin({
        filename: isProduction ? '[name].[contenthash].css' : '[name].css',
      }),
      new HtmlWebpackPlugin({
        template: path.resolve(__dirname, 'templates', 'index.html'),
        filename: 'index.html',
        inject: 'body',
        minify: isProduction
          ? {
              removeComments: true,
              collapseWhitespace: true,
              removeRedundantAttributes: true,
            }
          : false,
      }),
    ],
    optimization: {
      splitChunks: {
        chunks: 'all',
      },
      runtimeChunk: 'single',
    },
    performance: {
      hints: isProduction ? 'warning' : false,
    },
  };
};




# -------------------- C:\Users\jorda\OneDrive\Desktop\Quickbase Dev Work\KeystoneEmailParser\tailwind.config.js --------------------

// tailwind.config.js
module.exports = {
  content: ['./frontend/**/*.{js,jsx,ts,tsx}', './templates/**/*.html'], // Replace 'purge' with 'content'
  darkMode: 'media', // Change to 'media' or remove the line for default behavior
  theme: {
    extend: {},
  },
  plugins: [],
};




# -------------------- C:\Users\jorda\OneDrive\Desktop\Quickbase Dev Work\KeystoneEmailParser\package.json --------------------

{
  "name": "keystone-email-parser",
  "version": "1.0.0",
  "description": "Email parsing application with React frontend",
  "main": "static/js/main.js",
  "scripts": {
    "start": "concurrently \"python app.py\" \"webpack serve --mode development\"",
    "start:frontend": "webpack serve --mode development",
    "start:backend": "python app.py",
    "build": "webpack --mode development",
    "build:prod": "webpack --mode production",
    "watch": "webpack --watch --mode development",
    "clean": "del-cli static/dist/*",
    "lint": "eslint src/**/*.{js,jsx}",
    "test": "echo \"No tests specified\" && exit 0"
  },
  "dependencies": {
    "@babel/runtime": "^7.22.5",
    "@headlessui/react": "^2.2.0",
    "@reduxjs/toolkit": "^1.9.5",
    "axios": "^1.7.7",
    "dompurify": "^3.1.7",
    "file-saver": "^2.0.5",
    "jspdf": "^2.5.1",
    "length": "^0.0.1",
    "lodash": "^4.17.21",
    "lottie-web": "^5.12.2",
    "lucide-react": "^0.263.1",
    "prop-types": "^15.8.1",
    "react": "^18.2.0",
    "react-copy-to-clipboard": "^5.1.0",
    "react-dom": "^18.2.0",
    "react-focus-lock": "^2.13.2",
    "react-redux": "^8.1.1",
    "react-syntax-highlighter": "^15.6.1",
    "recharts": "^2.5.0",
    "redux": "^4.2.1",
    "redux-thunk": "^2.4.2",
    "socket.io-client": "^4.5.4",
    "uuid": "^11.0.2"
  },
  "devDependencies": {
    "@babel/core": "^7.26.0",
    "@babel/plugin-proposal-class-properties": "^7.18.6",
    "@babel/plugin-transform-runtime": "^7.25.9",
    "@babel/preset-env": "^7.26.0",
    "@babel/preset-react": "^7.25.9",
    "autoprefixer": "^10.4.20",
    "babel-loader": "^9.2.1",
    "clean-webpack-plugin": "^4.0.0",
    "concurrently": "^9.0.1",
    "cross-env": "^7.0.3",
    "css-loader": "^6.11.0",
    "del-cli": "^6.0.0",
    "eslint": "^8.56.0",
    "eslint-plugin-react": "^7.33.2",
    "eslint-plugin-react-hooks": "^4.6.0",
    "html-webpack-plugin": "^5.6.3",
    "mini-css-extract-plugin": "^2.9.1",
    "postcss": "^8.4.47",
    "postcss-loader": "^7.3.4",
    "rimraf": "^5.0.5",
    "style-loader": "^3.3.4",
    "tailwindcss": "^3.4.14",
    "webpack": "^5.95.0",
    "webpack-cli": "^5.1.4",
    "webpack-dev-server": "^4.15.0"
  },
  "browser": {
    "@": "./static",
    "@components": "./static/components",
    "@actions": "./static/js/actions",
    "@reducers": "./static/js/reducers",
    "@core": "./static/js/core",
    "@utils": "./static/js/utils",
    "@ui": "./static/js/ui",
    "@css": "./static/css"
  },
  "engines": {
    "node": ">=14.0.0",
    "npm": ">=6.0.0"
  },
  "browserslist": {
    "production": [
      ">0.2%",
      "not dead",
      "not op_mini all"
    ],
    "development": [
      "last 1 chrome version",
      "last 1 firefox version",
      "last 1 safari version"
    ]
  }
}




# -------------------- C:\Users\jorda\OneDrive\Desktop\Quickbase Dev Work\KeystoneEmailParser\jsconfig.json --------------------

// jsconfig.json
{
  "compilerOptions": {
    "baseUrl": ".",
    "paths": {
      "@/*": ["frontend/*"],
      "@components/*": ["frontend/components/*"],
      "@actions/*": ["frontend/actions/*"],
      "@reducers/*": ["frontend/reducers/*"],
      "@core/*": ["frontend/core/*"],
      "@utils/*": ["frontend/utils/*"],
      "@css/*": ["frontend/static/css/*"]
    }
  }
}




# -------------------- C:\Users\jorda\OneDrive\Desktop\Quickbase Dev Work\KeystoneEmailParser\app.py --------------------

# src/app.py
eventlet.monkey_patch()
# Set memory fraction to 90% of total GPU memory
if torch.cuda.is_available():
    torch.cuda.set_per_process_memory_fraction(0.9)
    # Clear the FFT plan cache for potentially faster CUDA operations
    torch.backends.cuda.cufft_plan_cache.clear()
def setup_logging() -> logging.Logger:
    formatter = json_log_formatter.JSONFormatter()
    json_handler = logging.StreamHandler()
    json_handler.setFormatter(formatter)
    logger = logging.getLogger("AppLogger")
    logger.addHandler(json_handler)
    logger.setLevel(logging.DEBUG)
    return logger
def init_app() -> (Flask, SocketIO):
    load_dotenv()
    app = Flask(__name__, static_folder='static', template_folder='templates')
    socketio = SocketIO(
        app,
        cors_allowed_origins="*",
        logger=True,
        engineio_logger=True,
        async_mode="eventlet",
    )
    return app, socketio
def setup_cache_dirs(logger: logging.Logger):
    cache_dir = str(Path("D:/AiHub"))
    os.environ["HF_HOME"] = cache_dir
    os.makedirs(cache_dir, exist_ok=True)
    logger.info("Cache directory set to: %s", cache_dir)
    if torch.cuda.is_available():
        logger.info("CUDA available: %s", torch.cuda.get_device_name(0))
    else:
        logger.info("CUDA not available, using CPU")
logger = setup_logging()
app, socketio = init_app()
def format_schema_output(formatted_data: Dict[str, Any]) -> str:
    output = []
    for section, fields in formatted_data.items():
        output.append(section)
        output.append("")
        for field, value in fields.items():
            output.append(f"{field}: {value}")
        output.append("")
    return "\n".join(output)
def make_serializable(obj: Any) -> Any:
    if isinstance(obj, (np.float32, np.float64)):
        return float(obj)
    elif isinstance(obj, (np.int32, np.int64)):
        return int(obj)
    elif isinstance(obj, dict):
        return {k: make_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [make_serializable(element) for element in obj]
    elif isinstance(obj, tuple):
        return tuple(make_serializable(element) for element in obj)
    elif isinstance(obj, set):
        return [make_serializable(element) for element in obj]
    elif isinstance(obj, (float, int, str)):
        return obj
    else:
        return str(obj)
def background_parse(
    sid: str,
    parser_option: str,
    email_content: str,
    document_image: Optional[Image.Image],
):
    # Create event loop for this thread
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        input_type = None
        if email_content and document_image:
            input_type = 'both'
        elif email_content:
            input_type = 'text'
        elif document_image:
            input_type = 'image'
        parser = ParserRegistry.get_parser(
            parser_option=ParserOption(parser_option),
            socketio=socketio,
            sid=sid,
        )
        if parser is None:
            socketio.emit(
                "parsing_error", {"error": f"Parser for option {parser_option} not found."}, room=sid
            )
            return
        with parser:
            if not parser.is_initialized:
                parser.initialize(input_type=input_type)
            if not parser.health_check():
                socketio.emit(
                    "parsing_error", {"error": "Parser health check failed."}, room=sid
                )
                return
            socketio.emit(
                "parsing_started", {"message": "Parsing started..."}, room=sid
            )
            stages = [
                {"stage": "Initializing parser", "progress": 10},
                {"stage": "Processing email content", "progress": 30},
                {"stage": "Extracting entities", "progress": 50},
                {"stage": "Finalizing results", "progress": 80},
                {"stage": "Completed", "progress": 100},
            ]
            for step in stages:
                socketio.emit(
                    "parsing_progress",
                    {"stage": step["stage"], "progress": step["progress"]},
                    room=sid,
                )
                socketio.sleep(1)
            result = parser.parse_email(
                email_content=email_content, document_image=document_image
            )
            # Handle structured_data and metadata
            structured_data = result.get("structured_data", {})
            metadata = result.get("metadata", {})
            # Optionally, log or use metadata
            logger.debug("Metadata from parsing: %s", metadata)
            if "email_metadata" in structured_data:
                formatted_text = format_schema_output(structured_data["email_metadata"])
                structured_data["formatted_schema"] = formatted_text
            serializable_result = make_serializable(structured_data)
            logger.debug("Serialized Result: %s", serializable_result)
            socketio.emit(
                "parsing_completed", {"result": serializable_result}, room=sid
            )
    except Exception as e:
        logger.error("Error during parsing: %s", e, exc_info=True)
        error_info = {
            "type": "unexpected_error",
            "message": f"An unexpected error occurred: {str(e)}",
            "time": datetime.now(timezone.utc).isoformat(),
            "exc_info": traceback.format_exc(),
        }
        serializable_error = make_serializable(error_info)
        socketio.emit("parsing_error", {"error": serializable_error}, room=sid)
    finally:
        loop.close()
@app.route("/", methods=["GET"])
def index():
    logger.info("Rendering index page.")
    return render_template("index.html")
@app.route("/favicon.ico")
def favicon_route():
    favicon_path = os.path.join(app.root_path, "static", "favicon.ico")
    if os.path.exists(favicon_path):
        return send_from_directory(
            os.path.join(app.root_path, "static"),
            "favicon.ico",
            mimetype="image/vnd.microsoft.icon",
        )
    logger.warning("favicon.ico not found at path: %s", favicon_path)
    return jsonify({"error_message": "favicon.ico not found."}), 404
@app.route("/parse_email", methods=["POST"])
def parse_email_route():
    email_content = request.form.get("email_content", "").strip()
    image_file = request.files.get("document_image")
    parser_option_str = request.form.get("parser_option", "").strip()
    socket_id = request.form.get("socket_id")
    if not email_content and not image_file:
        logger.warning("No email content or document image provided.")
        return (
            jsonify(
                {"error_message": "Please provide email content or document image"}
            ),
            400,
        )
    if not parser_option_str:
        logger.warning("No parser option selected.")
        return jsonify({"error_message": "Please select a parser option."}), 400
    sid = socket_id
    if not sid:
        logger.warning("No socket ID provided.")
        return jsonify({"error_message": "Socket ID not provided."}), 400
    logger.info("Received Socket ID: %s", sid)
    try:
        parser_option = ParserOption(parser_option_str)
    except ValueError:
        logger.warning("Invalid parser option selected: %s", parser_option_str)
        return (
            jsonify({"error_message": f"Invalid parser option: {parser_option_str}"}),
            400,
        )
    try:
        # Adjusted get_parser call without 'input_type'
        parser_config = ParserRegistry.get_parser(parser_option, socketio=socketio, sid=sid)
    except InitializationError as ie:
        logger.error("Parser initialization failed: %s", ie)
        return jsonify({"error_message": str(ie)}), 500
    except Exception as e:
        logger.error(
            "Unexpected error during parser initialization: %s", e, exc_info=True
        )
        return jsonify({"error_message": "Parser initialization failed"}), 500
    if parser_config is None:
        logger.error("Parser could not be initialized.")
        return jsonify({"error_message": "Parser could not be initialized"}), 500
    document_image = None
    if image_file:
        try:
            document_image = Image.open(io.BytesIO(image_file.read()))
        except Exception as e:
            logger.error("Image processing failed: %s", e, exc_info=True)
            return jsonify({"error_message": "Invalid image format"}), 400
    socketio.start_background_task(
        background_parse, sid, parser_option_str, email_content, document_image
    )
    logger.info("Parsing started for Socket ID: %s", sid)
    return jsonify({"message": "Parsing started"}), 202
@app.route("/health", methods=["GET"])
def health_check_route():
    try:
        status = ParserRegistry.health_check()
        logger.info("Health check passed.")
        return jsonify({"status": "healthy", "parsers": status}), 200
    except Exception as e:
        logger.error("Health check failed: %s", e, exc_info=True)
        return jsonify({"status": "unhealthy", "error": str(e)}), 500
@socketio.on("connect")
def handle_connect():
    sid = request.sid
    logger.info("Client connected: %s", sid)
@socketio.on("disconnect")
def handle_disconnect():
    sid = request.sid
    logger.info("Client disconnected: %s", sid)
@app.errorhandler(Exception)
def handle_exception(e: Exception):
    logger.error("Unhandled exception: %s", e, exc_info=True)
    return jsonify({"error_message": "An internal error occurred."}), 500
@app.errorhandler(404)
def page_not_found(e: Exception):
    logger.warning("404 error: %s not found. Exception: %s", request.url, e)
    return (
        jsonify({"error_message": "The requested URL was not found on the server."}),
        404,
    )
def signal_handler(_sig, _frame):
    try:
        logger.info("Shutdown initiated...")
        ParserRegistry.cleanup_parsers()
        logger.info("Cleanup completed successfully.")
    except Exception as e:
        logger.error("Error during cleanup: %s", e, exc_info=True)
    finally:
        sys.exit(0)
if __name__ == "__main__":
    try:
        logger.info("Starting application initialization...")
        setup_cache_dirs(logger)
        logger.info("Loading configuration...")
        Config.initialize()
        logger.info("Configuration loaded successfully")
        host = os.getenv("HOST", "127.0.0.1")
        port = int(os.getenv("PORT", "5000"))
        logger.info("Server will start on %s:%d", host, port)
        static_dir = os.path.join(app.root_path, "static")
        if not os.path.exists(static_dir):
            os.makedirs(static_dir)
            logger.info("Created static directory at %s", static_dir)
        ParserRegistry.initialize_parsers()
        logger.info("Parsers initialized successfully")
        # Register signal handlers for graceful shutdown
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        socketio.run(app, host=host, port=port, debug=True, use_reloader=False)
    except InitializationError as ie:
        logger.critical(
            "Parser initialization failed during startup: %s", ie, exc_info=True
        )
        try:
            ParserRegistry.cleanup_parsers()
        except Exception as cleanup_e:
            logger.error(
                "Error during cleanup after failed startup: %s",